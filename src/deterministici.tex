\chapter{Algoritmi deterministici}



\section{\BiMaxMatching}\label{sec:BiMaxMatching}
Sia $G=(V,E)$ un grafo non orientato. Un \emph{matching} (matrimonio) in $G$ è una selezione $X$ di lati in $E$ tale che su nessun vertice in $V$ incide più di un lato di $X$.
\MaxMatching è il problema di trovare il matching di cardinalità massima in un dato grafo.

\BiMaxMatching è una versione di \MaxMatching su grafi bipartiti, cioè in cui i vertici sono divisi in due classi e ogni lato incide su un vertice per classe: $G=(V_1,V_2,E)$ dove $V_1\cap V_2=\emptyset$ e $E\subseteq V_1\times V_2$. $\BiMaxMatching\in\PO$.

\popt{\BiMaxMatching}
{Grafo bipartito $G=(V_1,V_2,E)$}
{$X\subseteq E$}
{Determinare il matching di cardinalità massima in $G$}
{$X\subseteq E:\forall e_1,e_2\in X,~ e_1\cap e_2\neq\emptyset\impl e_1=e_2$}
{$\MAX$}
{$\card X$}

\begin{figure}
	\centering
	\input{img/matching_bipartito.tikz}
	\caption{Esempio di grafo bipartito. I lati colorati rappresentano un possibile matching.}
	\label{fig:graphmatching}
\end{figure}

Fissato un matching $M\subseteq E$, un lato $l\in E$ si dice \emph{occupato} se $l\in M$ e \emph{libero} se $l\notin M$.
Un vertice si dice \emph{esposto} se e solo se su di esso incidono solo lati liberi. Un cammino semplice si dice \emph{aumentante} rispetto a un matching se alterna lati liberi e occupati e inizia e termina su vertici esposti.
Quando un matching ha un cammino aumentante si può fare un \flang{flip}, cioè invertire l'appartenenza al matching dei lati del cammino aumentante.

\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\input{img/matching_aumentante_pre.tikz}
		\subcaption{Prima del flip.}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\input{img/matching_aumentante_post.tikz}
		\subcaption{Dopo il flip.}
	\end{subfigure}
	\caption{Esempio di cammino aumentante in un grafo bipartito.}
	\label{fig:augpaths}
\end{figure}

Vale il seguente teorema relativo ai cammini aumentanti per matching su grafi:
\begin{theorem}
	Sia $M$ un matching per un grafo $G$. Allora:
	\begin{equation*}
		\text{Esiste un cammino aumentante per $M$} \iff \text{$M$ non è massimo per $G$.}
	\end{equation*}
\end{theorem}
\begin{proof}~
	\begin{description}
		\item[$\Rightarrow$)] Applicando un flip al cammino aumentante si aumenta il matching di $1$.
		\item[$\Leftarrow$)] Se $M$ non è massimo, sia $M'$ un matching massimo per $G$ e sia $X:=(M\setminus M')\cup(M'\setminus M)$ (la differenza simmetrica di $M$ e $M'$).
			Su ogni vertice di $G$ possono incidere al più $2$ lati di $X$ (uno per ciascuno dei due matching).
			Nel grafo indotto da $X$, in ogni circuito l'appartenenza dei lati a $M$ e $M'$ è alternata e quindi il circuito è composto dallo stesso numero di lati di $M$ e di $M'$.
			Siccome però $M'$ ha più lati di $M$, esiste almeno un cammino semplice nel grafo indotto da $X$ che ha più lati in $M'$.
			Tale cammino alterna lati di $M$ e di $M'$ ed è aumentante rispetto a $M$ in $G$.
	\end{description}
\end{proof}

% TODO: è necessario discutere di come viene effettuata la visita per FindAugmenting e quale sia la sua complessità.
\SetKwFunction{FindAugmenting}{FindAugmenting}
\SetKwFunction{Flip}{Flip}
L'algoritmo \ref{alg:BiMaxMatching} risolve \BiMaxMatching trovando l'ottimo in tempo polinomiale.
La procedura \FindAugmenting tiene traccia dei vertici esposti e fa una visita in profondità del grafo a partire da uno di essi, alternando lati liberi e occupati, identificando un cammino aumentante.
La procedura \Flip esegue un flip del matching dato.

\begin{algorithm}
	\input{alg/BiMaxMatching.tex}
	\caption{Risoluzione polinomiale di \BiMaxMatching}
	\label{alg:BiMaxMatching}
\end{algorithm}

\begin{corollario}
	$\BiMaxMatching\in\PO$.
\end{corollario}

\PerfectMatching è il problema di decisione che si chiede se in un grafo bipartito esista un matching che coinvolge tutti i vertici.

\begin{corollario}
	$\PerfectMatching\in\P$.
\end{corollario}

% TODO: in relazione al todo precedente, spiegare perché l'algoritmo precedente non sia polinomiale su grafi generici e dire che in quel caso servono tecniche più avanzate per trovare cammini aumentanti.
Vale inoltre il seguente risultato, che non dimostriamo:
\begin{theorem}
	$\MaxMatching\in\PO$.
\end{theorem}



\section{\LoadBalancing}
Il problema \LoadBalancing (bilanciamento del carico) consiste nel dividere un insieme di task, ciascuno con la sua durata, tra le macchine di un insieme, in modo da minimizzare la durata complessiva (carico) impiegata dalla macchina più carica. Questo problema è \NPO-completo.

\popt
{\LoadBalancing}
{Durate $t_0,\dots,t_{n-1}\in\N^+$ per $n$ task, numero $m$ di macchine}
{Assegnamento di ciascun task a una macchina}
{Determinare l'assegnamento che minimizza la durata massima}
{Assegnamenti $x:n\to m$}
{$\MIN$}
{$L=\max_j L_j$, dove $L_j:=\sum_{i\in x^{-1}(j)} t_i$}


\subsection{\GreedyLoadBalancing}
Un algoritmo greedy può trovare una soluzione ammissibile per \LoadBalancing.
L'algoritmo esamina i task $t_0,t_1,\dots,t_{n-1}$ nell'ordine, assegnando ogni task alla macchina più scarica.
\GreedyLoadBalancing, se implementato con una coda con priorità che tenga conto della macchina dal carico minimo (effettuando $n$ operazioni di insert/update per i carichi di $m$ macchine), opera in tempo $O(n\log m)$.
L'algoritmo non ottiene, in generale, la soluzione ottima, ma ha la garanzia di non produrre più del doppio dell'ottimo, come dimostrato dal seguente teorema:

\begin{theorem}\label{thm:greedyloadbalancing}
	\GreedyLoadBalancing è un algoritmo 2-approssimante per \LoadBalancing.
\end{theorem}
Prima di dimostrare il teorema si consideri il seguente lemma:
\begin{lemma}\label{lem:load:ultimopasso}
	Sia $L\star$ il costo della soluzione ottima. Sia $\bar j$ tale che $L_{\bar j}=L\star$ e sia $\bar i$ l'ultimo task assegnato a $\bar j$. Allora:
	\begin{equation*}
		L_{\bar j}-t_{\bar i} \leq L\star
	\end{equation*}
\end{lemma}
\begin{proof}
	Il carico della macchina $\bar j$ prima dell'assegnamento $\bar i$ era $L_{\bar j}-t_{\bar i}$, il cui era minore di ogni altro carico per via di come l'algoritmo sceglie a chi assegnare.
	Indicato con $L\star_j$ il carico della macchina $j$ nella soluzione ottima, vale $L\star\geq\frac1m\sum_it_i$, essendo $mL\star\geq\sum_jL\star_j=\sum_it_i$.
	\begin{gather*}
		L_{\bar j}-t_{\bar i}\leq L_j \qquad\forall j \\
		\sum_j (L_{\bar j}-t_{\bar i})\leq\sum_j L_j=\sum_i t_i\\
		m(L_{\bar j}-t_{\bar i})\leq \sum_i t_i \\
		L_{\bar j}-t_{\bar i}\leq\frac1m\sum_j t_j\leq L\star
	\end{gather*}
\end{proof}
Si può ora procedere con la dimostrazione del teorema \ref{thm:greedyloadbalancing}:
\begin{proof}
	Si osservi che $L\star\geq\max_it_i$. Applicando il lemma \ref{lem:load:ultimopasso}:
	\begin{gather*}
		L=L_{\bar j}=\underbrace{L_{\bar j}-t_{\bar i}}_{\leq L\star}+\underbrace{t_{\bar i}}_{\leq L\star}\leq 2L\star \\[1ex]
		\frac L{L\star}\leq 2
	\end{gather*}
\end{proof}

\begin{corollario}
	$\LoadBalancing\in\gAPX2$.
\end{corollario}

Si dimostra che questo risultato sull'analisi di \GreedyLoadBalancing è tight:
\begin{theorem}
	Per ogni $\varepsilon>0$ esiste un input di \LoadBalancing su cui \GreedyLoadBalancing produce una soluzione $L$ tale che
	\begin{equation*}
		2-\varepsilon\leq\frac L{L\star}
	\end{equation*}
\end{theorem}
\begin{proof}
	Scegliamo un numero di macchine $m\geq\frac1\varepsilon$ e un numero di task $n=m(m-1)+1$. Questi task consistono, nell'ordine, in $n-1$ task di durata $1$ e un task da $m$. Naturalmente la soluzione ottima assegna unicamente il task da $m$ a una macchina, che risulta la più carica, quindi $L\star=m$.

	Per assegnare i task, l'algoritmo assegna un task da $1$ per ogni macchina ciclicamente finché arriva il task da $m$, che viene assegnato a una macchina di carico $m-1$ producendo un costo finale di $2m-1$. In questo caso si ha
	\begin{equation*}
		\frac L{L\star}=\frac{2m-1}{m}=2-\frac1m\geq2-\varepsilon
	\end{equation*}
\end{proof}


\subsection{\SortedGreedyBalance}
Un algoritmo \SortedGreedyBalance può ordinare i task in ordine decrescente prima di assegnarli come \GreedyLoadBalancing. Questo algoritmo ha costo in tempo di $O(n\log n+n\log m)$.

\begin{theorem}
	\SortedGreedyBalance è un algoritmo $\frac32$-approssimante per \LoadBalancing.
\end{theorem}
\begin{proof}
	Se $n\leq m$, a ciascuna macchina viene assegnato un solo task, quindi $L=L\star=\max_i t_i$. La tesi, pertanto, è ovvia.

	Se $n>m$, esiste una macchina che riceve due task.
	Il primo task assegnato a una macchina già carica è $m$.
	Poiché esistono $m$ task di durata superiore a $t_m$, anche nella soluzione ottima due di essi devono essere assegnati alla stessa macchina (principio della piccionaia), quindi vale:
	\begin{equation*}
		L\star\geq 2t_m \text.
	\end{equation*}

	Sia $\bar j$ una macchina con carico massimo al termine dell'esecuzione e $\bar i$ l'ultimo task assegnatovi.
	Se $\bar i$ è l'unico task di $\bar j$, allora la soluzione è ottima e la tesi è ovvia.
	In ogni altro caso si ha $\bar i \geq m$, da cui $t_{\bar i}\leq t_m$ e quindi $t_{\bar i}\leq t_m\leq \frac12 L\star$.
	\begin{gather*}
		L=\underbrace{L_{\bar j}-t_{\bar i}}_{\leq L\star}+\underbrace{t_{\bar i}}_{\leq \frac12 L\star}\leq L\star+\frac12 L\star=\frac32L\star
	\end{gather*}
\end{proof}

Valgono inoltre i seguenti teoremi riguardanti \LoadBalancing, che non dimostriamo.
\begin{theorem}~
	\begin{itemize}
		\item \SortedGreedyBalance è un algoritmo $\frac43$-approssimante per \LoadBalancing \cite{Graham:69:sortedgreedybalance};
		\item $\LoadBalancing\in\PTAS$;
		\item $\P\neq\NP\impl\LoadBalancing\notin\FPTAS$.
	\end{itemize}
\end{theorem}



\section{\CenterSelection}
Nel problema \CenterSelection è dato uno spazio metrico $\tuple{S,d}$.
Tra i punti si $S$ si selezionano centri $C\subseteq S$: ogni punto ha un centro di riferimento che consiste nel più vicino.
La massima distanza tra un punto di $S$ e il suo centro di riferimento è detta raggio di copertura ed è il valore che si vuole minimizzare.
\CenterSelection è un problema \NPO-completo.

\popt{\CenterSelection}
{Spazio metrico $S$ con distanza $d$, numero massimo di centri $k\in\N^+$}
{$C\subseteq S$}
{Selezionare al più $k$ centri che minimizzino il raggio di copertura}
{$C\subseteq S\mid \card C\leq k$}
{$\MIN$}
{$\rho(C) = \max_{x \in S} d(x, C)$}

Uno spazio metrico è un insieme $\Omega$ di punti su cui vale una funzione distanza $d:\Omega\times\Omega\to\R^{\geq0}$ tale che:
\begin{itemize}
	\item $\forall x\in S\quad d(x,x)=0$
	\item $\forall x,y\in S\quad d(x,y)=d(y,x)$
	\item $\forall x,y,z\in S\quad d(x,y)\leq d(x,z)+d(z,y)$ (disuguaglianza triangolare)
\end{itemize}
Sia $C\subseteq\Omega$ e $s\in\Omega$. Con abuso di notazione si indica con $d(s,C)$ la distanza minima tra il punto $s$ e i punti appartenenti a $C$, ossia $d(s,C):=\min_{c\in C} d(s,c)$.

\begin{defin}[tassellatura di Voronoi]
	Dato uno spazio metrico $\tuple{\Omega,d}$, sia $S\subseteq\Omega$ finito e $C\subseteq S$ un insieme di centri.
	Dato un punto $s\in S$ il centro di riferimento di $s$ è $\text{VC}_C(s):=\arg\min_{c\in C} d(s,c)$.
	Per ogni $c\in C$, la cella di Voronoi di centro $c$ è l'insieme $\text{VC}_C^{-1}(c)$ degli elementi di $S$ mappati in $c$ tramite $\text{VC}_C$.
\end{defin}

\begin{equation*}
	\text{VC}_C(s)=\arg\min_{c\in C} d(s,c)
\end{equation*}

\CenterSelection è il problema che riceve in input $S\subseteq_\text{fin}\Omega$ e $k\in\N^+$, ha come soluzioni ammissibili le selezioni di centri $C\subseteq S$ con $\card C\leq k$ e come obiettivo da minimizzare:
\begin{equation*}
	\rho(C)=\max_{s\in S} d(s,\text{VC}_C(s))
\end{equation*}

\subsection{\CenterSelectionPlus}
L'algoritmo \ref{alg:CenterSelectionPlus} non risolve \CenterSelection in quanto richiede un input $r$ aggiuntivo rispetto a quelli previsti dal problema, che consiste in una stima del raggio di copertura ottimo $\rho\star$.

\begin{algorithm}[ht]
	\caption{\CenterSelectionPlus}
	\label{alg:CenterSelectionPlus}
	\input{alg/CenterSelectionPlus.tex}
\end{algorithm}

\begin{theorem}
	Valgono le seguenti proprietà per \CenterSelectionPlus:
	\begin{enumerate}[(a)]
		\item \label{itm:centerselection:plus1} se l'algoritmo emette un output $C$, allora $C$ è una soluzione ammissibile con $\rho(C)\leq 2r$;
		\item \label{itm:centerselection:plus2} se $r\geq\rho\star$ allora l'algoritmo emette un output.
		\item \label{itm:centerselection:plus3} se $r<\frac{\rho\star}{2}$ allora l'algoritmo non produce un output.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\ref{itm:centerselection:plus1} L'output $C$ è ammissibile in quanto composto da elementi di $S$ e $\card C\leq k$.
	Poiché l'algoritmo termina quando sono stati eliminati tutti i punti di $S$, e ogni punto $s\in S$ viene eliminato quando per un centro $\bar s$ selezionato vale $d(s,\bar s)\leq 2r$, il raggio massimo da un punto al centro più vicino è minore o uguale a $2r$;

	\ref{itm:centerselection:plus2} Sia $C\star$ una soluzione ottima. Si consideri $\bar s\in S$ e sia $\bar c:=\text{VC}_{C\star}(\bar s)$ e $X_{\bar c}:=\text{VC}_{C\star}^{-1}(\bar c)$.
	Per qualunque punto $s\in X_{\bar c}$ vale, per disuguaglianza triangolare:
	\begin{equation*}
		d(s,\bar s) \leq d(s,\bar c)+d(\bar c,\bar s) \leq \rho\star + \rho\star = 2\rho\star \leq 2r \text.
	\end{equation*}
	Quindi dopo l'inserimento di $\bar s$ non rimane in $S$ alcun elemento di $X_{\bar c}$.
	Poiché la soluzione ottima contiene al più $k$ centri, dopo al più $k$ iterazioni tutti i punti di $S$ sono stati rimossi, e $\card C\leq k$.

	\ref{itm:centerselection:plus3} Se l'algoritmo emettesse un output $C$ per $r<\frac{\rho\star}{2}$, allora per il punto \ref{itm:centerselection:plus1} si avrebbe $\rho(C)\leq 2r<\rho\star$, il che è assurdo se $\rho\star$ è l'ottimo.
\end{proof}


Come rappresentato in figura \ref{fig:csplus_r_beh}, l'algoritmo ricevendo $r\geq\rho\star$ produce una soluzione ammissibile con approssimazione $\frac{2r}{\rho\star}$; se $r<\frac{\rho\star}{2}$ l'algoritmo non produce output; nei casi rimanenti il comportamento dell'algoritmo non è consistente.
Un algoritmo del genere può essere utilizzato sfruttando un'opportuna applicazione della ricerca dicotomica per la determinazione del valore ottimo di $r$.

\begin{figure}[ht]
	\centering
	\input{img/centerselectionplus.tikz}
	\caption{Comportamento di \CenterSelectionPlus.}
	\label{fig:csplus_r_beh}
\end{figure}


\subsection{\GreedyCenterSelection}
L'algoritmo \ref{algo:GreedyCenterSelection} è un algoritmo greedy per \CenterSelection.

\begin{algorithm}
	\caption{\GreedyCenterSelection}
	\input{alg/GreedyCenterSelection.tex}
	\label{algo:GreedyCenterSelection}
\end{algorithm}

\begin{theorem}
	\GreedyCenterSelection è un algoritmo $2$-approssimante per \CenterSelection.
\end{theorem}
\begin{proof}
	Si consideri una variante di \CenterSelectionPlus che, invece di eliminare punti di $S$, prenda in considerazione nella selezione di nuovi centri solo punti $s$ tali che $d(s,C)>2r$.
	Questo algoritmo \CenterSelectionPlus' è del tutto equivalente a \CenterSelectionPlus in quanto, selezionato un centro, i punti entro una distanza $2r$ da esso vengono ignorati per il resto della computazione.

	Per assurdo, supponiamo che l'output $C$ sia tale che $\rho(C)>2\rho\star$, ossia esiste $\hat s\in S$ tale che $d(\hat s,C)> 2\rho\star$.
	Consideriamo l'$i$-esima iterazione dell'algoritmo: sia $C_i$ l'insieme dei centri all'inizio dell'$i$-esima iterazione e $\bar s_i$ il centro da inserire.
	Per il funzionamento dell'algoritmo e poiché $C_i\subseteq C$, vale:
	\begin{equation*}
		d(\bar s_i,C_i)\geq d(\hat s,C_i)\geq d(\hat s, C)>2\rho\star
	\end{equation*}
	Quindi, il centro viene scelto tra quelli distanti almeno $2\rho\star$ dai centri attuali.
	L'algoritmo equivale quindi a un'esecuzione di \CenterSelectionPlus' con $r=\rho\star$.
	Ma per $r=\rho\star$ \CenterSelectionPlus' emette una $\frac{2\rho\star}{\rho\star}=2$-approssimazione, il che contraddice l'ipotesi di assurdo.
\end{proof}


\subsection{Inapprossimabilità di \CenterSelection}
\begin{theorem}
	Se $\P\neq\NP$, non esiste $\alpha<2$ tale che $\CenterSelection\in\gAPX\alpha$.
\end{theorem}
\begin{proof}
	Si consideri il problema di decisione \NP-completo \DominatingSet. Dato un grafo non orientato $G=(V,E)$, un dominating set per $G$ è un insieme di vertici $D\subseteq V$ se e solo se ogni vertice non appartenente a $D$ ha un adiacente in $D$: $\forall x\in(V\setminus D),\exists y\in D\mid \set{x,y}\in E$.

	\pdec{\DominatingSet}
	{Grafo non orientato $G=(V,E)$, $k\in\N^+$}
	{Determinare se esiste un dominating set per $G$ di cardinalità minore o uguale a $k$}

	Data un'istanza $((V,E),k)$ di \DominatingSet, costruiamo una istanza di \CenterSelection che ha per spazio l'insieme dei vertici, per numero massimo di centri il limite di cardinalità $k$ per il dominating set e per metrica la funzione
	\begin{equation*}
		d(x,y) =
		\begin{cases}
			0 \quad & \text{se } x = y                      \\
			1 \quad & \text{se } x\neq y\land\set{x,y}\in E \\
			2 \quad & \text{altrimenti}
		\end{cases}
	\end{equation*}
	Si noti che $d$ soddisfa la definizione di metrica.

	In una istanza di \CenterSelection derivata in questo modo, $\rho\star=2$ oppure $\rho\star=1$.
	Nel secondo caso l'insieme di centri scelti è un dominating set nell'istanza originale del problema.
	Infatti, sia $D\subseteq S$ la soluzione ottima per tale istanza. Vale:
	\begin{align*}
		\rho\star=1 & \coimpl\forall x\in(V\setminus D)\quad d(x,D)=1                             \\
		            & \coimpl\forall x \in (V \setminus D)\quad \exists y\in D\mid d(x,y)=1       \\
		            & \coimpl\forall x \in (V \setminus D)\quad \exists y\in D\mid \set{x,y}\in E \\
		            & \coimpl D\text{ è un dominating set}
	\end{align*}

	Per assurdo, supponiamo che esista un algoritmo polinomiale $\alpha$-approssimante per \CenterSelection, con $\alpha < 2$.
	Eseguendo tale algoritmo su un'istanza così costruita si ottiene un output $D$ tale che:
	\begin{gather*}
		1\leq\frac{\rho(D)}{\rho\star}\leq\alpha<2 \\
		\rho\star\leq\rho(D)\leq\alpha\cdot\rho\star<2\rho\star
	\end{gather*}

	Dato che $\rho\star=1\lor\rho\star=2$, allora vale esattamente una delle seguenti proposizioni:
	\begin{equation*}
		\begin{cases}
			1\leq\rho(D)<2\quad & \text{se } \rho\star=1 \\
			2\leq\rho(D)<4\quad & \text{se } \rho\star=2
		\end{cases}
	\end{equation*}
	Quindi, eseguito l'algoritmo, se $\rho(D)<2$ allora $\rho\star=1$ e la decisione per l'istanza originale di \DominatingSet è positiva; se $\rho(D)\geq2$ allora $\rho\star=2$ e la decisione è negativa.

	Poiché l'algoritmo agisce in tempo polinomiale, allora $\DominatingSet\in\P$, il che è assurdo se $\P\neq\NP$.
\end{proof}
Questa tecnica dimostrativa, consistente nel ridurre un problema \NP-completo e poterlo decidere in tempo polinomiale in ipotesi di assurdo discriminando di che intervallo fa parte la soluzione, è detta tecnica di riduzione con produzione di gap (\flang{gap-producing reduction}).



\section{\MinSetCover}\label{sec:SetCover}
Si definisce funzione armonica la funzione $H:\N^+\to\R$ tale che
\begin{equation*}
	H(n)=\sum_{k=1}^n \frac 1k
\end{equation*}

% TODO: dimostrare in un'appendice (vedi vecchi appunti)
Vale la seguente proprietà per la funzione armonica:
\begin{theorem}
	$\ln(n+1)\leq H(n)\leq 1+\ln n$.
\end{theorem}

Nel problema \MinSetCover sono dati insiemi, ciascuno con il suo peso, la cui unione è un insieme universo.
Il problema consiste nel trovare una selezione degli insiemi la cui unione copra l'intero insieme universo e il cui costo complessivo sia minimo.
\MinSetCover è un problema \NPO-completo.

\popt{\MinSetCover}
{$S_1,S_2,\dots,S_m\subseteq U$ tali che $\cup_{i=1}^m S_i=U$ e pesi $w_1,\dots,w_m$ con $w_i \in\Q^+~\forall i$}
{$C\subseteq\set{S_1,\dots,S_n}$}
{Determinare una selezione di insiemi che minimizzi il costo complessivo}
{$C$ tale che $\cup_{i\in C}S_i=U$}
{$\MIN$}
{$w:=\sum_{i:S_i\in C} w_i$}


\subsection{\GreedySetCover}
\begin{algorithm}[ht]
	\caption{\GreedySetCover}
	\label{algo:greedysetcover}
	\input{alg/GreedySetCover.tex}
\end{algorithm}
L'algoritmo \ref{algo:greedysetcover} costruisce polinomialmente una soluzione per \MinSetCover, scegliendo a ogni iterazione il sottoinsieme di input che minimizza il rapporto tra il suo peso e il numero di elementi che esso aggiunge all'output parziale.

Ogni elemento $s\in U$ viene inserito nell'output parziale in qualche iterazione $j$ con l'aggiunta di un sottoinsieme $S_j$. Definiamo quindi
\begin{equation*}
	c_u = \frac{w_j}{\card{S_j\cap R_j}}
\end{equation*}
il costo della copertura del singolo elemento di $U$, avvenuta tramite l'aggiunta di $S_j$ durante la $j$-esima iterazione.

\begin{lemma}\label{lem:gsetcov_w_sum_c_u}
	\begin{equation*}
		w=\sum_{u\in U} c_u
	\end{equation*}
\end{lemma}
\begin{proof}
	Si noti che gli insiemi $S_j\cap R_j$, dove $S_j$ è l'insieme di input scelto al passo $j$ e $R_j$ è l'insieme degli elementi dell'universo rimasti da selezionare al passo $j$, costituiscono una partizione di $U$. Infatti, l'algoritmo termina solo dopo aver esaurito gli elementi di $U$, e ogni insieme $S_j\cap R_j$ aggiunge unicamente nuovi elementi.

	Sia $w_j$ il costo dell'insieme $S_j$ aggiunto al passo $j$. Allora
	\begin{equation*}
		w = \sum_j w_j=\sum_j\sum_{s\in S_j\cap R_j} c_s=\sum_{u\in U} c_u
	\end{equation*}
\end{proof}
\begin{lemma}\label{lem:gsetcov_cu_leq_harmoskwk}
	\begin{equation*}
		\forall k\in\set{1,\dots,m} \quad\sum_{s\in S_k} c_s\leq H(\card{S_k}) \cdot w_k
	\end{equation*}
\end{lemma}
\begin{proof}
	Sia $S_k=\set{s_1,s_2,\dots,s_d}$, dove gli elementi sono elencati in ordine di copertura.

	Prima che un elemento $s_i$ venga coperto dall'inserimento di un insieme $S_{k'}$, gli elementi di $S_k$ ancora da inserire spaziano almeno da $s_i$ a $s_d$, quindi:
	\begin{equation*}
		\card{S_k\cap R_j}\geq d-i+1 \text.
	\end{equation*}
	Quindi
	\begin{equation*}
		c_{s_i}=\frac{w_{k'}}{\card{S_{k'}\cap R_j}}
		\leq\frac{w_k}{\card{S_k\cap R_j}}
		\leq\frac{w_k}{d-i+1} \text.
	\end{equation*}
	E, di conseguenza
	\begin{align*}
		\sum_{s\in S_k} c_s & =c_{s_1}+c_{s_2}+c_{s_3}\dots+c_{s_d}                            \\
		                    & \leq \frac{w_k}{d-1+1}+\frac{w_k}{d-2+1}+\dots+\frac{w_k}{d-d+1} \\
		                    & \leq \frac{w_k}{d}+\frac{w_k}{d-1}+\dots+\frac{w_k}{1}           \\
		                    & = w_k\left(1 + \frac{1}{2} + \dots + \frac{1}{d}\right)          \\
		                    & = w_k\cdot H(\card{S_k})
	\end{align*}
\end{proof}

\begin{theorem}
	Sia $M=\max_i\card{S_i}$. \GreedySetCover è $H(M)$-approssimante per \MinSetCover.
\end{theorem}
\begin{proof}
	Sia $w\star:=\sum_{i:S_i\in C\star} w_i$.
	Eseguito l'algoritmo, in virtù del lemma \ref{lem:gsetcov_cu_leq_harmoskwk} vale, per qualunque $i$:
	\begin{equation*}
		w_i\geq\frac{\sum_{s\in S_i} c_s}{H(\card{S_i})}\geq\frac{\sum_{s\in S_i} c_s}{H(M)}
	\end{equation*}
	Essendo $C\star$ una copertura e applicando il lemma \ref{lem:gsetcov_w_sum_c_u}:
	\begin{equation*}
		\sum_{S_i\in C\star}\sum_{s\in S_i} c_s \geq \sum_{s\in U} c_s = w
	\end{equation*}
	Applicando queste due osservazioni:
	\begin{gather*}
		w\star = \sum_{i:S_i\in C\star} w_i \geq \sum_{i:S_i\in C\star} \frac{\sum_{s\in S_i} c_s}{H(M)} \geq \frac{w}{H(M)} \\
		\frac{w}{w\star} \leq H(M)
	\end{gather*}
\end{proof}

Inoltre vale:
\begin{equation*}
	H(M)\leq H(\card U) = O(\log \card U)
\end{equation*}
Ergo:
\begin{corollario}
	\GreedySetCover è un algoritmo $O(\log n)$-approssimante per \MinSetCover, dove $n$ è la cardinalità dell'insieme universo.
\end{corollario}

Per quanto riguarda l'ottimalità di questo bound:
\begin{theorem}
	Per ogni $\varepsilon>0$, \GreedySetCover non è $(O(\log n)-\varepsilon)$-approssimante per \MinSetCover.
\end{theorem}
\begin{proof}
	% TODO: è necessario essere più precisi in questa costruzione: si può fare con qualunque n? È necessaria una potenza di 2? Gli insiemi S finali possono contenere un solo elemento o sempre almeno 2? Etc.
	Fissati $\varepsilon$ e $n$ (sia per semplicità $n=2^k$ per qualche $k\in\N^+,k>2$), si consideri l'input per \MinSetCover mostrato in figura \ref{fig:setcover_tightness}.
	L'input è costituito da due insiemi disgiunti $A$ e $B$ di costo $1+\varepsilon$ e cardinalità $n/2$; e $\log_2 n$ insiemi disgiunti $S_1,S_2,\dots,S_{\log_2 n}$, di cardinalità rispettive $n/2,n/4,\dots$ e costo $1$. In ciascun insieme $S_i$, metà degli elementi è contenuta in $A$ e l'altra metà in $B$.

	\begin{figure}[ht]
		\centering
		\input{img/greedysetcover_badinput.tikz}
		\caption{Esempio di input "cattivo" per $n=8$}
		\label{fig:setcover_tightness}
	\end{figure}

	L'algoritmo sceglie nell'ordine gli insiemi $S_i$ in quanto il costo di aggiungerli è, per ogni iterazione $j$, $\frac{1}{n/2^j}$, contro un costo di $\frac{1+\varepsilon}{n/2^j}$ per scegliere $A$ o $B$.
	Questo porta un costo complessivo di $\log_2 n$. La soluzione ottima tuttavia è naturalmente quella composta dagli insiemi $A$ e $B$, che ha un costo di $2+2\varepsilon$. Il rapporto tra le due è necessariamente logaritmico.
\end{proof}



\section{\VertexCover}
Una copertura per un grafo non orientato $G=(V,E)$ è un insieme di vertici $X\subseteq V$ tale che ogni lato di $E$ incide su un vertice in $X$.
Il problema \VertexCover associa a ogni vertice un peso e cerca la copertura di costo complessivo minimo.
\VertexCover è \NPO-completo.

\popt{\VertexCover}
{Grafo non orientato $G=(V,E)$ e pesi $w_1,\dots,w_n$, con $n=\card V$ e $w_i\in\Q^+~\forall i$}
{$X\subseteq V$}
{Determinare una copertura di $G$ di costo minimo}
{$X\subseteq V$ tale che $\forall e\in E ~ e\cap X\neq\emptyset$}
{$\MIN$}
{$w=\sum_{i\in X} w_i$}

Si considerino i problemi di decisione associati a \VertexCover e \MinSetCover.
Un'istanza $\tuple{G=(V,E),\angle{w_i}_{i\in V}}$ di \VertexCover può essere convertita in tempo polinomiale in una di \MinSetCover scegliendo come insiemi gli insiemi dei lati incidenti su ogni vertice $i$:
\begin{equation*}
	S_i=\set{e\in E\mid i\in e}
\end{equation*}
L'insieme universo è $E$ e i pesi sono quelli del vertice relativo a ogni insieme.

\begin{theorem}
	Sia $D$ il grado massimo di un grafo di input a \VertexCover. \VertexCover è $H(D)$-approssimabile.
\end{theorem}


\subsection{\PricedVertexCover}
Si consideri un'istanza di \VertexCover formata dal grafo $G=(V,E)$ e i pesi $\angle{w_i}_{i\in V}$.
$\angle{P_e}_{e\in E}$ è un \emph{assegnamento di prezzi} sui lati.
Un assegnamento $\angle{P_e}_{e\in E}$ si dice \emph{equo} se e solo se
\begin{equation*}
	\forall i\in V\quad\sum_{e\ni i} P_e\leq w_i \text.
\end{equation*}
Un assegnamento si dice \emph{stretto} su un vertice $i$ se e solo se
\begin{equation*}
	\forall i\in V\quad\sum_{e\ni i} P_e = w_i \text.
\end{equation*}

\begin{lemma}\label{lem:vcov_pricing_eq_sum_p_e_w_opt}
	Se $\angle{P_e}_{e\in E}$ è equo allora
	\begin{equation*}
		\sum_{e\in E} P_e \leq w\star
	\end{equation*}
	dove $w\star$ il costo ottimo per l'istanza di \VertexCover.
\end{lemma}
\begin{proof}
	Sia $X\star\subseteq V$ una soluzione ottima. Poiché $\angle{P_e}_{e\in E}$ è equo:
	\begin{equation*}
		\sum_{i\in X\star} \sum_{e\ni i} P_e \leq \sum_{i\in X\star} w_i = w\star \text.
	\end{equation*}
	Ogni lato del grafo incide su un vertice di $X\star$, quindi
	\begin{equation*}
		\sum_{e\in E} P_e \leq \sum_{i\in X\star} \sum_{e\ni i} P_e \leq w\star \text.
	\end{equation*}
\end{proof}

L'algoritmo \ref{algo:PricedVertexCover} usa una tecnica di pricing per costruire una soluzione per \VertexCover.
\begin{algorithm}
	\caption{\PricedVertexCover}
	\label{algo:PricedVertexCover}
	\input{alg/PricedVertexCover.tex}
\end{algorithm}

\begin{lemma}\label{lem:pvcov_w_le_w_sum_P_e}
	L'algoritmo \ref{algo:PricedVertexCover} produce una soluzione $S$ ammissibile per \VertexCover. Inoltre al termine dell'esecuzione vale:
	\begin{equation*}
		w \leq 2 \sum_{e \in E} P_e \text.
	\end{equation*}
\end{lemma}
\begin{proof}
	All'uscita dal ciclo, non esistono lati $\set{i,j}$ tali che $\angle{P_e}$ non è stretto né su $i$ né su $j$.
	Quindi l'insieme $S$ dei vertici su cui $\angle{P_e}$ è stretto è una copertura.

	Essendo $S$ una soluzione ammissibile, per definizione:
	\begin{equation*}
		w = \sum_{i\in S} w_i \text.
	\end{equation*}
	Poiché $S$ contiene solo vertici su cui $\angle{P_e}$ è stretto:
	\begin{equation*}
		\forall i\in S \quad w_i = \sum_{e\ni i} P_e \text.
	\end{equation*}
	Ergo
	\begin{equation*}
		w = \sum_{i\in S} \sum_{e\ni i} P_e \text.
	\end{equation*}
	Poiché un lato compare nella somma al più $2$ volte:
	\begin{equation*}
		w \leq 2 \sum_{e\in E} P_e \text.
	\end{equation*}
\end{proof}

\begin{theorem}
	\PricedVertexCover è un algoritmo $2$-approssimante per \VertexCover.
\end{theorem}
\begin{proof}
	\begin{equation*}
		\frac{w}{w\star} \underset{\text{lemma \ref{lem:pvcov_w_le_w_sum_P_e}}}{\leq}
		\frac{2\sum_{e\in E} P_e}{w\star} \underset{\text{lemma \ref{lem:vcov_pricing_eq_sum_p_e_w_opt}}}{\leq}
		\frac{2\sum_{e\in E} P_e}{\sum_{e\in E} P_e} = 2 \text.
	\end{equation*}
\end{proof}


\subsection{\VertexCover tramite arrotondamento di \LinearProgramming}
Si consideri il problema di programmazione lineare e la sua versione vincolata all'integrità della soluzione, il problema di programmazione lineare intera.

\popt{\LinearProgramming}
{Sistema $Ax\geq b$, con $A\in\Q^{m\times n},b\in\Q^m$ ($x$ incognito), vettore $c\in\Q^n$.}
{Assegnamenti per $x$ in $\Q^n$}
{Determinare il vettore $x$ che minimizza la funzione obiettivo}
{$x\in\Q^n\mid Ax\geq b$}
{$\MIN$}
{Funzione obiettivo $c\trans x$}

\popt{\IntegerLinearProgramming}
{Sistema $Ax\geq b$, con $A\in\Q^{m\times n},b\in\Q^m$ ($x$ incognito), vettore $c\in\Q^n$.}
{Assegnamenti per $x$ in $\Z^n$}
{Determinare il vettore $x$ che minimizza la funzione obiettivo}
{$x\in\Z^n\mid Ax\geq b$}
{$\MIN$}
{Funzione obiettivo $c\trans x$}

Per lo stesso input, una soluzione ottima di \IntegerLinearProgramming è ammissibile, ma non necessariamente ottima, per \LinearProgramming.
Viceversa una soluzione ottima per \LinearProgramming può non essere ammissibile per un'istanza di \IntegerLinearProgramming dallo stesso input.

\LinearProgramming è un problema appartenente a \PO (l'ottimo può essere trovato polinomialmente ad esempio con l'algoritmo di Karmarkar \cite{Karmarkar:84:LP}), mentre \IntegerLinearProgramming è \NPO-completo.

Data un'istanza di \VertexCover $\tuple{G=(V,E),\angle{w_i}_{i\in V}}$, con $n:=\card V$ e $m:=\card E$, si consideri l'istanza di \IntegerLinearProgramming in cui il sistema di input è così costruito:
\begin{equation*}
	\begin{cases}
		x_i\geq 0     & \qquad \forall i\in V         \\
		x_i\leq 1     & \qquad \forall i\in V         \\
		x_i+x_j\geq 1 & \qquad \forall \set{i,j}\in E \\
	\end{cases}
\end{equation*}
e la funzione obiettivo è
\begin{equation*}
	w = \min\sum_{i\in V} w_i x_i
\end{equation*}

Una soluzione $x$ dell'istanza di \IntegerLinearProgramming costruita si può interpretare come una soluzione di \VertexCover in cui $x_i=1\iff i\in X$.

Si consideri l'istanza di \LinearProgramming ottenuta rilassando il vincolo di integrità dell'istanza precedente.
Una soluzione ottima $x$ di tale istanza può essere calcolata in tempo polinomiale, ma non è, in generale, ammissibile per la sua versione intera.
Si consideri il vettore $r$, ottenuto dall'arrotondamento di $x$, ossia, per ciascun $i\in n$:
\begin{equation*}
	r_i:= \begin{cases}
		1 & \quad x_i\geq\frac{1}{2} \\
		0 & \quad x_i<\frac{1}{2}
	\end{cases}
\end{equation*}

\begin{lemma}\label{lem:ilp_r_ammiss}
	Il vettore $r$ è una soluzione ammissibile di \IntegerLinearProgramming.
\end{lemma}
\begin{proof}
	Per definizione, $0\leq r\leq 1$. Se non fosse $r_i+r_j\geq 1~\forall \set{i,j}\in E$, siano $\bar i,\bar j$ tali che $r_{\bar i}+r_{\bar j}<1$.
	Allora $r_{\bar i}=r_{\bar j}=0$. Per definizione di $r$ si ha $x_{\bar i}<\frac 12$ e $x_{\bar j}<\frac 12$.
	Ma allora $x_{\bar i}+x_{\bar j}<1$, il che contraddice l'ammissibilità di $x$.
\end{proof}

\begin{lemma}\label{lem:ilp_r_i_leq_2_x_i}
	\begin{equation*}
		\forall i\in V \qquad r_i \leq 2x_i
	\end{equation*}
\end{lemma}
\begin{proof}
	Se $r_i=0$ la disuguaglianza è ovvia;
	se $r_i=1$ allora, $x_i\geq \frac 12$ e $2x_i\geq 1=r_i$.
\end{proof}

\begin{theorem}\label{lem:ilp_appr}
	L'insieme $\set{i\in V\mid r_i=1}$ è una $2$-approssimazione per \VertexCover.
\end{theorem}
\begin{proof}
	Sia $w:=\sum{i\in V} w_i r_i$ il costo della soluzione di \VertexCover indotta dall'istanza arrotondata di \LinearProgramming, e sia $w\star$ la soluzione ottima. Si denoti con $w\star_{\text{LP}}$ il costo ottimo dell'istanza di \LinearProgramming e $w\star_{\text{ILP}}$ quello di \IntegerLinearProgramming.
	Applicando il lemma \ref{lem:ilp_r_i_leq_2_x_i}:
	\begin{equation*}
		w = \sum_{i\in V} w_i r_i \leq 2\sum_{i\in V} w_i x_i = 2w\star_{\text{LP}} \leq 2w\star_{\text{ILP}} = w\star
	\end{equation*}
\end{proof}





\section{\DisjointPaths}
Dato un grafo orientato su cui sono selezionati un numero di vertici \flang{source} e un numero di rispettivi vertici \flang{target} (potenzialmente con molteplicità), il problema \DisjointPaths si pone l'obiettivo di massimizzare il numero di coppie source-target connettibili da un cammino usando ogni arco un numero massimo di $c$ volte, dove $c$ è un dato parametro detto di congestione.
Il problema prende il nome dalla sua variante con $c=1$, in cui i cammini non hanno archi in comune.
\DisjointPaths è \NPO-completo.

\popt{\DisjointPaths}
{Grafo orientato $G=(V,E)$, vertici $\angle{s_i}_{i\in k}$ e $\angle{t_i}_{i\in k}$ e un parametro $c\in\N^+$}
{$I\subseteq k$, cammini $\angle{\pi_i}_{i\in I}$, con $\pi_i:=s_i\leadsto t_i$}
{Determinare il massimo numero di coppie $s_i,t_i$ che si possono connettere con un cammino, usando un arco al più $c$ volte complessivamente}
{$I\subseteq k$, cammini $\angle{\pi_i}_{i\in I}$ tali che nessun arco $e\in E$ appartenga ai cammini più di $c$ volte}
{$\MAX$}
{$\card I$}


\subsection{\PricedDisjointPaths}
L'algoritmo \ref{algo:PricedDisjointPaths} usa una tecnica di pricing in cui viene definita una funzione di costo $l:E\to\Q^+$ per gli archi, estendibile ai cammini $\tuple{x_0,x_1\dots,x_{t-1},x_t}$ con $l(\tuple{x_0,x_1,\dots,x_{t-1},x_t}):=l((x_0,x_1))+\dots+l((x_{t-1},x_t))$.
L'algoritmo fa inoltre uso di un valore $\beta$ che, come si vedrà, può essere calcolato in modo da ottimizzare il risultato.
\SetKwFunction{MinPath}{MinPath}
La procedura \MinPath restituisce, in tempo polinomiale, un cammino di costo minimo e l'indice $i$ dei vertici $s_i$ e $t_i$ che collega, con $i\notin I$. Se un cammino del genere non esiste, la procedura restituisce un cammino vuoto.
L'algoritmo produce una soluzione ammissibile per \DisjointPaths, dal momento che $P$ contiene solo cammini con archi utilizzati al più $c$ volte (dopo $c$ volte un arco viene eliminato) e che collegano, per definizione di \MinPath, vertici non ancora collegati.
\PricedDisjointPaths ha un costo polinomiale: utilizzando ad esempio Floyd-Warshall la procedura \MinPath può essere implementata in $O(\card V^3)$ e viene ripetuta un massimo di $k$ volte, per un costo totale in tempo di $O(k\card V^3)$.

\begin{algorithm}
	\caption{\PricedDisjointPaths}
	\label{algo:PricedDisjointPaths}
	\input{alg/PricedDisjointPaths.tex}
\end{algorithm}

A una data iterazione dell'algoritmo, un cammino $\pi$ si dice \emph{corto} se e solo se $l(\pi)<\beta^c$.
Un cammino $\pi$ si dice \emph{utile} se e solo se collega una coppia $i\notin I$.

Finché esistono cammini corti e utili, l'algoritmo seleziona uno di essi a ogni iterazione.
Quando nessun cammino è corto e utile, l'esecuzione si ferma oppure iniziano a venire selezionati cammini lunghi (i.e. non corti).
Si consideri la prima iterazione $\bar t$ in cui non esistono cammini corti e utili, o il termine dell'esecuzione se tale iterazione non esiste.
Sia $\bar l$ la funzione di costo in tale iterazione e $\bar I$ l'insieme degli indici dei vertici collegati da cammini.

\begin{lemma}\label{lem:priceddpaths_non_included_non_short}
	Se all'iterazione $\bar t$ la coppia $\tuple{s_i,t_i}$ non è stata collegata dalla soluzione corrente, allora il costo del relativo cammino ottimo $\pi\star_i$ è maggiore o uguale a $\beta^c$:
	\begin{equation*}
		\bar l(\pi\star_i)\geq\beta^c\qquad\forall i\notin I
	\end{equation*}
\end{lemma}
\begin{proof}
	Se fosse $\bar l(\pi\star_i)<\beta^c$, allora $\pi\star$ sarebbe corto e utile, pertanto sarebbe stato selezionato prima dell'iterazione $\bar t$.
\end{proof}

\begin{lemma}\label{lem:priceddpaths_sum_l_a_leq_bc_i_m}
	Sia $m:=\card E$.
	\begin{equation*}
		\sum_{e\in E}\bar l(e) \leq \beta^{c+1}\card{\bar I} + m
	\end{equation*}
\end{lemma}
\begin{proof}~
	\begin{itemize}
		\item Alla prima iterazione, $\sum_{e\in E} l_0(e) = \sum_{e\in E} 1 = m$.
		\item Al termine di ogni iterazione $j<\bar t$, si modificano i valori $l_j$ in valori $l_{j+1}$ così scelti:
		      \begin{equation*}
			      l_{j+1}(e) =
			      \begin{cases}
				      l_j(e)            & \quad\text{se } e\notin\pi_i \\
				      \beta\cdot l_j(e) & \quad\text{se } e\in\pi_i
			      \end{cases}
		      \end{equation*}
		      Si consideri la differenza tra i pesi complessivi all'iterazione $j$ e quelli all'iterazione $j+1$:
		      \begin{align*}
			      \sum_{e\in E} l_{j+1}(e) - \sum_{e\in E} l_j(e) & = \sum_{e\in E} (l_{j+1}(e)-l_j(e))   \\
			                                                      & = \sum_{e\in\pi}(\beta l_j(e)-l_j(e)) \\
			                                                      & = \sum_{e\in\pi} (\beta-1)l_j(e)      \\
			                                                      & \leq \beta\sum_{e\in\pi}l_j(e)
		      \end{align*}
		      Tale valore è al più uguale a $\beta^{c+1}$, essendo il cammino $\pi$ corto perché $j<\bar t$.
	\end{itemize}
	Quindi, all'inizio dell'iterazione $\bar t$, a un costo iniziale di $m$ sono state aggiunte $\card{\bar I}$ variazioni (una per ogni iterazione e quindi aggiunta di cammini a $I$) di al più $\beta^c$ l'una, ergo:
	\begin{equation*}
		\sum_{e\in E} \bar l(e) \leq \beta^{c+1}\card{\bar I}+m \text.
	\end{equation*}
\end{proof}

\begin{corollario}\label{cor:priceddpaths_cor_1}
	\begin{equation*}
		\sum_{i\in I\star\setminus I} \bar l(\pi_i\star) \geq \beta^c \card{I\star\setminus I} \text.
	\end{equation*}
\end{corollario}
\begin{proof}
	Ottenuto dal lemma \ref{lem:priceddpaths_non_included_non_short} sommando per i valori in $I\star\setminus I$.
\end{proof}

\begin{corollario}\label{cor:priceddpaths_cor_2}
	\begin{equation*}
		\sum_{i\in I\star\setminus I} \bar l(\pi\star_i) \leq c(\beta^{c+1}\card{\bar I}+m) \text.
	\end{equation*}
\end{corollario}
\begin{proof}
	\begin{align*}
		\sum_{i\in I\star\setminus I} \bar l(\pi\star_i) & \leq \sum_{i\in I\star} \bar l(\pi\star_i)                                                                                     \\
		                                                 & \leq c \sum_{e\in E} \bar l(e)             &  & \text{ogni arco è usato al più $c$ volte nella soluzione ammissibile $I\star$} \\
		                                                 & \leq c (\beta^{c+1}\card{\bar I}+m)        &  & \text{dal lemma \ref{lem:priceddpaths_sum_l_a_leq_bc_i_m}}
	\end{align*}
\end{proof}

\begin{theorem}\label{thm:priceddpaths_approx}
	\PricedDisjointPaths è un algoritmo $(1+c(\beta+\beta^{-c}m))$-approssimante per \DisjointPaths.
	Se $\beta=m^{\frac{1}{c+1}}$, \PricedDisjointPaths fornisce una $(1+2cm^{\frac{1}{c+1}})$-approssimazione.
\end{theorem}
\begin{proof}
	Sia $I\star$ la soluzione ottima e $I$ la soluzione prodotta da \PricedDisjointPaths.
	\begin{align*}
		\beta^c\card{I\star} & = \beta^c\card{I\star\cap I}+\beta^c\card{I\star\setminus I}                                                                                  \\
		                     & \leq \beta^c\card{I\star\cap I} + \sum_{i\in I\star\setminus I} \bar l(\pi\star_i) &  & \text{per il corollario \ref{cor:priceddpaths_cor_1}} \\
		                     & \leq \beta^c\card I + \sum_{i\in I\star\setminus I} \bar l(\pi\star_i)                                                                        \\
		                     & \leq \beta^c\card I + c(\beta^{c+1}\card{\bar I}+m)                                &  & \text{per il corollario \ref{cor:priceddpaths_cor_2}} \\
		                     & \leq \beta^c\card I + c(\beta^{c+1}\card I+m)                                      &  & \text{essendo $\bar I\subseteq I$} \text.
	\end{align*}
	Dividendo per $\beta^c$:
	\begin{align*}
		\card{I\star} & \leq \card I+c\beta\card I+c\beta^{-c}m                                                   \\
		              & \leq \card I+c\beta\card I+c\beta^{-c}m\card I &  & \text{essendo $\card I\geq 1$} \text.
	\end{align*}
	Dividendo per $\card I$:
	\begin{equation*}
		\frac{\card{I^*}}{\card I} \leq 1+c\beta+c\beta^{-c}m = 1+c(\beta+\beta^{-c}m)
	\end{equation*}
	% TODO: dimostrare in un'appendice
	Questo valore è minimizzato per $\beta=m^{\frac{1}{c+1}}$, ottenendo:
	\begin{align*}
		\frac{\card{I^*}}{\card I} & \leq 1+c\left(m^{\frac{1}{c+1}}+m^{\frac{-c}{c+1}}m\right) \\
		                           & = 1+c\left(m^{\frac{1}{c+1}}+m^{\frac{-c+c+1}{c+1}}\right) \\
		                           & = 1+2cm^{\frac{1}{c+1}}
	\end{align*}
	L'analisi dimostra che le sole prime $\bar t$ iterazioni producono una $(1+2cm^{\frac{1}{c+1}})$-approssimazione. Dal momento che, come mostrato, $\bar I\subseteq I$, l'algoritmo al termine dell'esecuzione produce una soluzione quantomeno non peggiore.
\end{proof}



\section{\TravelingSalesman}
Il problema del commesso viaggiatore, o \TravelingSalesman (problem, abbreviato in TSP), è uno dei problemi più famosi della teoria dei grafi.
Il problema consiste nel trovare un circuito hamiltoniano di costo minimo in un grafo pesato sugli archi.
Dato un grafo non orientato, un \emph{circuito hamiltoniano} è un circuito che passa per ogni vertice del grafo una e una sola volta.
\TravelingSalesman è \NPO-completo.

\popt{\TravelingSalesman}
{Grafo non orientato $G=(V,E)$, pesi $\angle{\delta_e}_{e\in E}$}
{$\pi\subseteq E$ ordinato}
{Determinare il circuito hamiltoniano di minor costo}
{$\pi$ forma un circuito hamiltoniano}
{$\MIN$}
{$\sum_{e\in\pi} \delta_e$}

Con un abuso di notazione useremo $\delta$ per indicare il costo del suo argomento, inteso come la somma dei pesi degli archi che lo compongono.

L'algoritmo di Christofides, che descriveremo a breve, mette insieme una serie di risultati, problemi e algoritmi e trova un'approssimazione della soluzione ottima di \TravelingSalesman. Introduciamo ora tali nozioni.


\subsection{Requisiti}

\subsubsection{Circuiti euleriani}
Dato un grafo non orientato, un \emph{circuito euleriano} è un circuito che include ogni lato del grafo una e una sola volta. Si noti che possono esistere circuiti euleriani che includono un vertice multiple volte.

Il problema di trovare un circuito euleriano in un grafo (o multigrafo) è stato formalizzato da Eulero a partire dal problema dei ponti di Könisberg, che chiedeva se fosse possibile attraversare tutti i ponti della città una volta e tornare al punto di partenza.
Eulero dimostra una condizione necessaria e sufficiente per l'esistenza di un cammino euleriano in un multigrafo:
\begin{theorem}[di Eulero]\label{thm:eulero}
	Un multigrafo ammette un circuito euleriano se e solo se è connesso e tutti i suoi vertici hanno grado pari.
\end{theorem}

Un risultato fondamentale inerente alla parità del grado dei vertici è il lemma delle strette di mano (\flang{handshaking lemma}):
\begin{lemma}[delle strette di mano]\label{lem:handshaking}
	In ogni grafo, il numero di vertici di grado dispari è pari.
\end{lemma}
\begin{proof}
	Dal momento che ogni arco aumenta il grado di due vertici, la somma dei gradi di tutti i vertici è pari. Ma una somma di interi è pari se e solo se il numero di addendi dispari è pari.
\end{proof}

\subsubsection{Il TSP metrico su cricca}\label{subsub:tsp:criccametrica}

\paragraph{Il TSP su cricca} A partire da un'istanza di \TravelingSalesman composta dal grafo $G=(V,E)$ e i pesi $\angle{\delta_e}_{e\in E}$, si consideri l'istanza composta dalla cricca $K=\left(V,\binom V 2\right)$ e dai pesi $\angle{\bar\delta}_{e\in\binom V 2}$ così definiti:
\begin{equation*}
	\bar\delta_e = \begin{cases}
		\delta_e                 & \quad e \in E    \\
		1+\sum_{e\in E} \delta_e & \quad e \notin E
	\end{cases}
\end{equation*}
Trovando la soluzione ottima di \TravelingSalesman su questa istanza, è facile determinare se il circuito hamiltoniano trovato è una soluzione per il grafo originale.
Poiché ogni arco viene percorso al più una volta (altrimenti i vertici verrebbero ripetuti nel circuito), se il costo totale è al più $\sum_{e\in E} \delta_e$ allora nessun arco assente nel grafo originale è stato usato, e la soluzione è ottima anche per l'istanza originale.
Viceversa, se il costo totale è almeno $1+\sum_{e\in E} \delta_e$ allora non è stato possibile trovare un circuito euleriano che non usasse un arco assente nell'istanza originale, e quindi questa non ha soluzione.
Si può quindi semplificare la risoluzione di \TravelingSalesman senza perdita di generalità limitandosi a risolvere il problema nel caso di cricche.

\paragraph{Il TSP metrico} Semplifichiamo ora il problema, imponendo una caratteristica "metrica" alla funzione dei pesi, cioè la disuguaglianza triangolare:
\begin{equation*}
	\forall i,j,k\in V \quad \delta_{\set{i,j}} \leq \delta_{\set{i,k}} + \delta_{\set{k,j}} \text.
\end{equation*}
Come vedremo, solo grazie a questa assunzione è possibile costruire algoritmi approssimanti per \TravelingSalesman.

\subsubsection{L'albero ricoprente minimo}
Dato un grafo non orientato connesso, un albero ricoprente è un sottografo che sia un albero e mantenga tutti i vertici.
Se il grafo è pesato sui lati, un albero ricoprente minimo minimizza la somma dei costi dei lati che mantiene.
\MinimumSpanningTree (MST) è il problema di ottimizzazione che cerca un albero ricoprente minimo in un grafo.
Il problema è risolvibile esattamente in tempo polinomiale, ad esempio dall'algoritmo di Kruskal in tempo $O(m\log n)$.

\popt{\MinimumSpanningTree}
{Grafo connesso $G=(V,E)$, pesi $\angle{\delta_e}_{e\in E}$}
{Albero $T=(V',E')$}
{Trovare un albero ricoprente minimo per $G$}
{$T$ è un albero tale che $V'=V$}
{$\MIN$}
{$\sum_{e\in E'} \delta_e$}

\subsubsection{\MinimumWeightPerfectMatching}
Dato un grafo non orientato con un numero pari di vertici, un \emph{matching perfetto} è un matching (vedi \ref{sec:BiMaxMatching}) che coinvolge tutti i vertici.
In un grafo pesato sugli archi, \MinimumWeightPerfectMatching è il problema di trovare il matching perfetto a costo complessivo minimo.
\MinimumWeightPerfectMatching è risolvibile esattamente in tempo polinomiale, ad esempio con il \flang{blooming algorithm} in tempo $O(m\log n)$.

\popt{\MinimumWeightPerfectMatching}
{Grafo $G=(V,E)$ tale che $\card V$ è pari, pesi $\angle{\delta_e}_{e\in E}$}
{Matching $M\subseteq E$}
{Determinare un matching perfetto di costo minimo}
{$M$ è un matching perfetto su $G$}
{$\MIN$}
{$\sum_{e\in M} \delta_e$}


\subsection{L'algoritmo di Christofides}
L'algoritmo di Christofides per TSP metrico su cricca applica i concetti precedentemente descritti manipolando diverse strutture fino a ottenere un cammino hamiltoniano. Ricevendo in input una cricca $G=(V,E)$ e pesi $\angle{\delta_e}_{e\in E}$, i passi dell'algoritmo sono i seguenti:
\begin{enumerate}
	\item identificare un albero ricoprente minimo $T$ per $G$;
	\item sia $D$ il grafo indotto su $G$ dall'insieme dei suoi vertici che hanno grado dispari in $T$.
	      Questi sono in numero pari per il lemma \ref{lem:handshaking}. È quindi possibile identificare un matching perfetto minimo $M$ su $D$;
	\item sia $H$ il multigrafo indotto su $G$ dall'unione disgiunta dei lati di $T$ ed $M$. In $H$ tutti i vertici hanno grado pari, poiché quelli che avevano grado dispari in $T$ hanno un nuovo arco grazie a $M$. In virtù del lemma \ref{thm:eulero} si può trovare un circuito euleriano $\pi$ in $H$;
	\item trasformare il circuito euleriano $\pi$, valido anche per $G$, in un hamiltoniano $\tilde\pi$.
	      Ogni volta che $\pi$ passa su un vertice $v$ una seconda volta tramite i lati $\set{a,v}$ e $\set{v,b}$, è sufficiente "saltare" tale vertice sostituendo in $\tilde\pi$ i due lati con il lato $\set{a,b}$ (esistente in quanto si sta lavorando sulla cricca originale), con conseguente diminuzione del costo in virtù della disuguaglianza triangolare.
\end{enumerate}

L'analisi dell'algoritmo ci porta a dimostrare un risultato di approssimazione rispetto al TSP metrico su cricca.
\begin{lemma}\label{lem:chri_spanning}
	In una cricca $G$ pesata metricamente sui lati, il costo $\delta(T)$ di un albero ricoprente minimo $T$ non è peggiore del costo minimo $\delta\star$ di un cammino hamiltoniano su $G$:
	\begin{equation*}
		\delta(T) \leq \delta\star
	\end{equation*}
\end{lemma}
\begin{proof}
	Sia $\pi\star$ un circuito hamiltoniano ottimo e sia $e\in\pi\star$.
	Il grafo indotto da $\pi\star\setminus e$ è un albero ricoprente. Pertanto:
	\begin{equation*}
		\delta(T) \leq \delta(\pi\star\setminus e) \leq\delta\star \text.
	\end{equation*}
\end{proof}

\begin{lemma}\label{lem:chri_matching}
	\begin{equation*}
		\delta(M) \leq \frac 1 2 \delta\star
	\end{equation*}
\end{lemma}
\begin{proof}
	Si consideri un circuito hamiltoniano ottimo $\pi\star$. Poiché è hamiltoniano, questo comprende i vertici di $D$. Cortocircuitando $\pi\star$ sui soli vertici di $D$, si ottiene un circuito di peso complessivo minore o uguale a quello di $\pi\star$ (per la disuguaglianza triangolare) e che ha un numero di lati pari (poiché i vertici di $D$ sono pari). Alternando i lati di questo circuito, si formano due matching $M_1$ e $M_2$ perfetti su $D$. Ma poiché $M$ è il matching perfetto di costo minimo per $D$, vale:
	\begin{equation*}
		\delta(\pi\star)\geq\delta(M_1)+\delta(M_2)\geq 2\delta(M) \text. \qedhere
	\end{equation*}
\end{proof}

\begin{theorem}
	L'algoritmo di Christofides è $\frac 3 2$-approssimante per il TSP metrico su cricca.
\end{theorem}
\begin{proof}
	Per la costruzione del circuito hamiltoniano $\tilde\pi$ a partire dal circuito euleriano $\pi$ e grazie ai lemmi sopra dimostrati:
	\begin{equation*}
		\delta(\tilde\pi)\leq\delta(\pi) = \delta(T)+\delta(M)\leq
		\underbrace{\delta\star}_{\text{lemma \ref{lem:chri_matching}}} + \underbrace{\frac{\delta\star}{2}}_{\text{lemma \ref{lem:chri_spanning}}} =
		\frac 3 2 \delta\star \text.
	\end{equation*}
\end{proof}

\begin{theorem}
	Per ogni $\varepsilon>0$ esiste un input del TSP metrico su cricca su cui l'algoritmo di Christofides produce una soluzione $\pi$ tale che
	\begin{equation*}
		\frac 3 2-\varepsilon \leq \frac{\delta(\pi)}{\delta\star}
	\end{equation*}
\end{theorem}
\begin{proof}
	Dato $n$ pari e $\varepsilon\in(0,1)$, si consideri il grafo in figura \ref{fig:christotight}, in cui gli archi sono etichettati con i loro pesi. Si estenda il grafo a una cricca $G$, in cui ogni lato $\set{u,v}$ non rappresentato ha come peso il peso del cammino minimo tra $u$ e $v$ sul grafo originale.

	\begin{figure}[ht]
		\centering
		\input{img/christo_tight.tikz}
		\caption{Esempio di input "cattivo" per l'algoritmo di Christofides.}
		\label{fig:christotight}
	\end{figure}

	L'algoritmo di Christofides identifica l'albero ricoprente $T$ indotto da tutti i lati di peso $1$, quindi $\delta(T)=n-1$.
	L'algoritmo seleziona poi $D=\set{v_1,v_n}$ e su di esso il matching $M$ composto dal solo lato $\set{v_1,v_n}$ di peso $\delta(M)=(1+\varepsilon)\left(\frac n2-1\right)+1$.
	Sul grafo indotto dall'unione $H$ di $T$ e $M$ l'algoritmo costruisce il circuito euleriano, nonché hamiltoniano, composto dall'unione degli archi di peso $1$ e l'arco che compone $M$.
	Il costo $\delta$ del circuito costruito è quindi:
	\begin{equation*}
		\delta = n-1 + (1+\varepsilon)\left(\frac n2-1\right)+1 = \frac32n+\varepsilon\left(\frac n2-1\right)-1 \text.
	\end{equation*}
	Ma il circuito hamiltoniano ottimo è quello che, a partire da $v_1$, percorre tutti gli archi di peso $1+\varepsilon$ che uniscono vertici di indice dispari, l'arco $\set{v_{n-1},v_n}$, tutti gli archi di peso $1+\varepsilon$ di indice pari e infine l'arco $\set{v_1,v_2}$.
	Il costo $\delta\star$ di tale circuito è:
	\begin{equation*}
		\delta\star = 2(1+\varepsilon)\left(\frac n2-1\right) + 2 = n+\varepsilon(n-2) \text.
	\end{equation*}
	Quindi
	\begin{equation*}
		\frac{\delta}{\delta\star} = \frac{\frac32n+\varepsilon\left(\frac n2-1\right)-1}{n+\varepsilon(n-2)}
	\end{equation*}
	Che è definitivamente\footnote{Precisamente, la disequazione è vera per $n\geq 2+\frac{1-2\varepsilon}{\varepsilon^2}$.} maggiore o uguale a $\frac 3 2-\varepsilon$.

\end{proof}


\subsection{Inapprossimabilità di \TravelingSalesman}
Abbandonando l'ipotesi di disuguaglianza triangolare della funzione $\delta$, il problema \TravelingSalesman è inapprossimabile.
Al fine di dimostrare ciò ricordiamo che decidere se un grafo contiene un cammino hamiltoniano è un problema \NP-completo.

\begin{theorem}
	Se $\P\neq\NP$, non esiste alcun $\alpha>1$ tale che \TravelingSalesman sia $\alpha$-approssimabile.
\end{theorem}
\begin{proof}
	Fissato $\alpha>1$, sia $G=(V,E)$ un grafo non orientato in cui tutti i lati hanno peso $1$.
	Sia $G'$ la sua estensione a cricca, ottenuta con il metodo citato nel paragrafo \ref{subsub:tsp:criccametrica}, ossia in cui la funzione peso $\delta$ è così definita:
	\begin{equation*}
		\delta(x,y) = \begin{cases}
			1          & \quad \set{x,y}\in E    \\
			\alpha n+1 & \quad \set{x,y}\notin E
		\end{cases}
	\end{equation*}
	Se $G$ ammette un circuito hamiltoniano, allora tale circuito è ammesso anche in $G'$ e ha costo $n<\alpha n$.
	Viceversa, se $G$ non ammette un circuito hamiltoniano allora ogni circuito hamiltoniano in $G'$ ha costo di almeno $\alpha n+1$.

	Se esiste un algoritmo $\alpha$-approssimante per \TravelingSalesman, questo trova in tempo polinomiale un circuito hamiltoniano in $G'$ di costo al più $\alpha n$. Ma un tale circuito esiste se e solo se esiste un cammino hamiltoniano in $G$. L'algoritmo decide quindi in tempo polinomiale se un grafo $G$ ammette un cammino hamiltoniano, il che è impossibile se $\P\neq\NP$.
\end{proof}



\section{\texorpdfstring{$2$}{2}-\LoadBalancing}
$2$-\LoadBalancing è una specializzazione di \LoadBalancing in cui il numero di macchine $m$ è uguale a $2$.
Questo problema è anche chiamato \MinimumPartition in quanto è equivalente al bilanciare una partizione insiemistica di due elementi.
Il problema è \NPO-completo.

L'algoritmo \ref{algo:partitionbalance} è un algoritmo \PTAS-caratterizzante per $2$-\LoadBalancing.
L'algoritmo opera in tempo polinomiale rispetto alla dimensione dell'input ed esponenziale rispetto a $\varepsilon$.

\begin{algorithm}
	\caption{Algoritmo \PTAS per $2$-\LoadBalancing.}
	\label{algo:partitionbalance}
	\input{alg/PartitionBalance.tex}
\end{algorithm}

\begin{theorem}
	Dato $\varepsilon>0$, l'algoritmo \ref{algo:partitionbalance} produce in tempo polinomiale in $n$ una $(1+\varepsilon)$-approssimazione per $2$-\LoadBalancing.
\end{theorem}
\begin{proof}
	Sia $L:=\frac12\sum_{i=1}^n t_i$.
	Il costo ottimo è almeno $L$, perciò se $\varepsilon\geq1$, assegnando tutti i task alla stessa macchina si ottiene un costo di $\sum_{i=1}^n t_i=2L\leq(1+\varepsilon)L\star$. Qualunque altro assegnamento produrrebbe una soluzione migliore, e quindi altrettanto buona, per questo caso.

	Se $\varepsilon<1$, si consideri lo stato delle macchine al termine dell'esecuzione e sia $L_1\geq L_2$ senza perdita di generalità.
	Sia $t_h$ l'ultimo task assegnato alla macchina $1$.
	\begin{itemize}
		% TODO: inserire considerazioni della dispensa aggiuntiva del prof
		\item Se $h\leq k$, $t_h$ è stato assegnato nella fase esaustiva e pertanto la soluzione costruita è ottima.
		\item Se $h>k$, allora $t_h$ è assegnato nella fase greedy, quindi, se $L_2'$ è il carico della macchina $2$ all'assegnamento di $t_h$, vale:
		      \begin{equation*}
			      L_1-t_h\leq L_2'\leq L_2
		      \end{equation*}
		      Quindi, ricordando che $L_1+L_2=2L$:
		      \begin{align}
			      L_1-t_h  & \leq L_2 \nonumber                    \\
			      2L_1-t_h & \leq L \nonumber                      \\
			      L_1      & \leq L+\frac{t_h}{2} \label{eq:2lb:1}
		      \end{align}
		      Inoltre vale
		      \begin{equation}\label{eq:2lb:2}
			      2L = \underbrace{t_0+t_1+\dots+t_k}_{\geq t_hk}+\underbrace{\dots+t_h+\dots+t_{n-1}}_{\geq t_h} \geq t_h(k+1)
		      \end{equation}
		      Ergo:
		      \begin{align*}
			      \frac{L_1}{L\star} & \leq \frac{L+\frac{t_h}{2}}{L\star}        &  & \text{per la \ref{eq:2lb:1}} \\
			                         & \leq \frac{L+\frac{t_h}{2}}{L}             &  & L\star\geq L                 \\
			                         & = 1+\frac{t_h}{2L}                                                           \\
			                         & \leq 1+\frac{t_h}{t_h(k+1)}                &  & \text{per la \ref{eq:2lb:2}} \\
			                         & = 1+\frac{1}{k+1}                                                            \\
			                         & \leq 1+\frac{1}{\frac{1}{\varepsilon}-1+1} &  & k\geq\frac{1}{\varepsilon}-1 \\
			                         & = \varepsilon+1 \text.
		      \end{align*}
	\end{itemize}
	Considerate le complessità delle operazioni di ordinamento, della ricerca esaustiva delle $2^k$ combinazioni per la parte ottima, e della coda per la parte greedy, l'algoritmo ha tempo d'esecuzione $O(n\log n+2^{\frac{1}{\varepsilon}}n)$.
\end{proof}



\section{\Knapsack}
\Knapsack, il problema dello zaino, è un celebre problema di ottimizzazione.
Dati uno zaino di capacità $W$ e degli oggetti, ciascuno con un valore e un volume (o peso), il problema consiste nel selezionare un gruppo di oggetti in modo che il loro volume non ecceda la capacità dello zaino ma il loro valore complessivo sia massimo.
\Knapsack è un problema \NPO-completo.

% TODO: bisogna pensare a un modo furbo di indicizzare gli oggetti di modo che la colonna i gestisca l'oggetto di indice i. Occhio però alla notazione angle, che non supporta facilmente gli indici da 1
\popt{\Knapsack}
{$n$ oggetti con valori $\angle{v_i}_{i\in n}\in\N^+$ e pesi $\angle{w_i}_{i\in n}\in\N^+$, capacità $W\in N$}
{$I\subseteq n$}
{Determinare l'insieme di oggetti di valore complessivo maggiore il cui peso complessivo non superi $W$}
{$\sum_{i\in I}w_i\leq W$}
{$\MAX$}
{$\sum_{i\in I} v_i$}


\subsection{\DynamicKnapsack}
Il problema può essere risolto in modo esatto dall'algoritmo \DynamicKnapsack, basato su programmazione dinamica.
L'algoritmo costruisce una matrice in cui costruisce le soluzioni ottime per istanze di complessità minore di quella in input, usandole per calcolare la soluzione ottima complessiva.

L'elemento $A[i,w]$ della matrice $A\in{\N^+}^{(n+1)\times (W+1)}$ è associato semanticamente al massimo valore ottenibile con i primi $i$ oggetti con una capacità massima di $w$.
Gli elementi $A[0,w]$, per ogni $w\in W+1$, sono vincolati al valore $0$ dal momento che nessun oggetto può essere utilizzato, mentre i restanti valori sono così calcolati:
\begin{equation*}
	A[i+1,w] =
	\begin{cases}
		A[i,w]                       & \quad w_i>w     \\
		\max(A[i,w], A[i,w-w_i]+v_i) & \quad w_i\leq w
	\end{cases}
\end{equation*}
L'algoritmo valuta quindi se l'oggetto $i+1$ ha volume compatibile con la capacità massima: in caso negativo reitera la soluzione che non usa l'ultimo oggetto, in caso positivo sceglie la migliore tra tale soluzione e quella che fa uso dell'ultimo oggetto (costruita aggiungendolo alla soluzione ottima dell'istanza con capacità $w-w_i$).

Riempita la tabella, l'algoritmo può ricostruire la soluzione che produce il valore ottimo $A[n,W]$.
L'algoritmo opera in tempo pseudopolinomiale, in quanto opera su una matrice di dimensione lineare nel valore di $W$, che è esponenziale nella lunghezza dell'input.


\subsection{\WeightDynamicKnapsack}
Una variante dell'algoritmo \DynamicKnapsack costruisce una matrice $B\in{\N^+}^{(n+1)\times (\sum_{i\in n} v_i)}$ in cui l'elemento $B[i,v]$ rappresenta il minimo peso necessario per raggiungere un valore di $v$ usando i primi $i$ oggetti.

L'elemento $B[0,0]$ è posto uguale a $0$. Gli elementi $B[0,v]$, per ogni $v\in\sum_{i\in n} v_i$, sono vincolati al valore $+\infty$ dal momento che il valore $v$ è irraggiungibile. I restanti valori sono così calcolati:
\begin{equation*}
	B[i+1,v] = \min(B[i,v], w_i+B[i,\max(v-v_i,0)])
\end{equation*}
L'algoritmo sceglie quindi la migliore soluzione tra quella che non fa uso dell'oggetto $i+1$-esimo e quella che lo aggiunge alla soluzione che utilizza i primi $i$.

Un algoritmo che usa questa matrice può trovare la soluzione ottima cercando sulla riga $n$ la colonna di indice massimo il cui valore non supera $W$. Un tale algoritmo sarebbe tuttavia poco efficiente per via dell'utilizzo di una matrice con un tale numero di colonne.

L'algoritmo può essere migliorato, a costo di perdere l'esattezza, introducendo una normalizzazione che ridimensiona i valori in base al valore massimo $v_{\max}$.
L'algoritmo risultante approssima a piacere in tempo polinomiale.

Sia $\pi=\tuple{\angle{v_i}_{i\in n},\angle{w_i}_{i\in n},W}$ un'istanza di \Knapsack. Si rimuovano eventuali pesi $i$ tali che $w_i>W$, che non contribuiscono ad alcuna soluzione ammissibile.
Dato un valore $\varepsilon\in(0,1]$, si definisce il valore di scala $\theta$ come
\begin{equation*}
	\theta:=\frac{\varepsilon v_{\max}}{2n} \text.
\end{equation*}
Si considerino le istanze $\bar\pi:=\tuple{\angle{\bar v_i=\ceil{\frac{v_i}{\theta}}\theta}_{i\in n},\angle{w_i}_{i\in n},W}$ e $\hat\pi:=\tuple{\angle{\hat v_i=\ceil{\frac{v_i}{\theta}}}_{i\in n},\angle{w_i}_{i\in n},W}$, che scalano in modo diverso i valori e lasciano invariati pesi e capacità.
Siano $\bar I\star$ e $\hat I\star$ le rispettive soluzioni ottime e $\bar v\star$ e $\hat v\star$ i loro valori.

\begin{lemma}\label{lem:knap_barhat}
	$\bar I\star = \hat I\star \land \bar v\star = \theta\hat v\star$.
\end{lemma}
\begin{proof}
	La funzione obiettivo del problema $\bar\pi$ non è altro che una dilatazione di fattore $\theta$ di quella del problema $\hat\pi$, pertanto le soluzioni ottime sono le stesse e i valori ottimi subiscono una dilatazione.
\end{proof}

\begin{lemma}
	Sia $I$ una soluzione ammissibile per $\pi$. Allora
	\begin{equation*}
		(1+\varepsilon)\sum_{i\in\hat I\star} v_i \geq \sum_{i\in I} v_i
	\end{equation*}
\end{lemma}
\begin{proof}
	\begin{align*}
		\sum_{i\in I} v_i & \leq \sum_{i\in I} \bar v_i                                          &  & \quad \text{arrotondamento per eccesso}      \\
		                  & \leq \sum_{ i\in\bar I\star} \bar v_i                                                                                  \\
		                  & = \sum_{ i\in\hat I\star} \bar v_i                                   &  & \quad \text{per lemma \ref{lem:knap_barhat}} \\
		                  & \leq \sum_{i\in\hat I\star} (v_i+\theta)                             &  & \quad \text{in quanto
		$\bar v_i=\ceil{\frac{v_i}{\theta}}\theta\leq\left(\frac{v_i}{\theta}+1\right)\theta=v_i+\theta$}                                          \\
		                  & \leq n\theta+\sum_{i\in\hat I\star} v_i                                                                                \\
		                  & = \frac{\varepsilon v_{\max}}{2} + \sum_{i\in\hat I\star} v_i \text.
	\end{align*}
	Quindi
	\begin{equation}\label{eq:knap:1}
		\sum_{i\in I} v_i \leq \frac{\varepsilon v_{\max}}{2} + \sum_{i\in\hat I\star} v_i
	\end{equation}

	\noindent Si consideri la soluzione ammissibile composta da un solo peso di valore massimo. Applicando (\ref{eq:knap:1}):
	\begin{gather}
		v_{\max} \leq \frac{\varepsilon v_{\max}}{2} + \sum_{i\in\hat I\star} v_i \leq \frac{v_{\max}}{2} + \sum_{i\in\hat I\star} v_i \nonumber \\
		\sum_{i\in\hat I\star} v_i \geq \frac{v_{\max}}{2} \label{eq:knap:2}
	\end{gather}
	Tornando alla generica soluzione ammissibile $I$ e applicando (\ref{eq:knap:1}) e (\ref{eq:knap:2}):
	\begin{gather*}
		\sum_{i\in I} v_i \leq \frac{\varepsilon v_{\max}}{2} + \sum_{i\in\hat I\star} v_i
		\leq \varepsilon\sum_{i\in\hat I\star} v_i + \sum_{i\in\hat I\star} v_i = (1+\varepsilon)\sum_{i\in\hat I\star} v_i
	\end{gather*}
\end{proof}
\begin{corollario}\label{corol:knapapprox}
	$\displaystyle(1+\varepsilon)\sum_{i\in\hat I\star} v_i \geq v\star$
\end{corollario}

\begin{theorem}
	$\Knapsack\in\FPTAS$.
\end{theorem}
\begin{proof}
	Risolvendo il problema $\hat\pi$ ottimamente con $\WeightDynamicKnapsack$ si trova una soluzione $\hat I\star$ che ha valore, nella funzione obiettivo originale, di $\sum_{i\in\hat I\star} v_i$. In virtù del corollario \ref{corol:knapapprox} tale valore produce un rapporto di approssimazione di $1+\varepsilon$.

	Per verificare che questo metodo sia effettivamente polinomiale in $n$ e $\frac{1}{\varepsilon}$, valutiamo il numero di colonne della matrice $B$.
	Nel caso peggiore tutti i pesi sono uguali a $\hat v_{\max}$, pertanto esso sarà $n \hat v_{\max}$.
	Ma vale
	\begin{equation*}
		\hat v_{\max}=\ceil{\frac{v_{\max}}{\theta}}=\ceil{\frac{2nv_{\max}}{\varepsilon v_{\max}}}=\ceil{\frac{2n}{\varepsilon}}\leq\frac{2n}{\varepsilon}+1 \text,
	\end{equation*}
	quindi la dimensione complessiva della matrice è
	\begin{equation*}
		n\cdot n\hat v_{\max}\leq n^2\left(\frac{2n}{\varepsilon}+1\right)=\frac{2n^3}{\varepsilon}+n^2
	\end{equation*}
\end{proof}

Non si può applicare alla variante originale di \DynamicKnapsack un approccio analogo a quello sopra descritto, in quanto la compressione porterebbe a un'approssimazione, invece che nei valori, nei pesi, perdendo quindi la garanzia di ammissibilità delle soluzioni ottenute.
